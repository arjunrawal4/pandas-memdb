{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import resource \n",
    "import pickle as pickle\n",
    "import parquet\n",
    "import pyarrow\n",
    "import feather\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREC = 2\n",
    "\n",
    "def convert_to_categorical(col):\n",
    "    if (col.nunique() / len(col.index)) < 0.2:\n",
    "        cat_dtype = pd.api.types.CategoricalDtype(ordered=True)\n",
    "        col = col.astype(cat_dtype, copy=False)\n",
    "    return col\n",
    "\n",
    "\n",
    "def shrink_floats(col):\n",
    "    if col.dtypes == 'float64':\n",
    "        col = (col * (10**PREC)).astype('int64', copy=False)\n",
    "    return col\n",
    "\n",
    "def shrink_integers(col):\n",
    "    if col.dtypes == 'int64':\n",
    "        min_val = col.min()\n",
    "        max_val = col.max()\n",
    "#         print(c, df[c].max(),df[c].min())\n",
    "#         print(df[c].memory_usage(deep=True)) \n",
    "        if max_val <= 255 and min_val >= 0:\n",
    "            col = col.astype('uint8', copy=False)\n",
    "        elif max_val <= 65535 and min_val >= 0:\n",
    "            col = col.astype('uint16', copy=False)            \n",
    "        elif max_val <= 4294967295 and min_val >= 0:\n",
    "            col = col.astype('uint32', copy=False)            \n",
    "        elif max_val <= 127 and min_val >= -128:\n",
    "            col = col.astype('int8', copy=False) \n",
    "        elif max_val <= 32767 and min_val >= -32768:\n",
    "            col = col.astype('int16', copy=False)\n",
    "        elif max_val <= 2147483647 and min_val >= -2147483648:\n",
    "            col = col.astype('int32', copy=False)\n",
    "        else:\n",
    "            print(\"not worth changing types\")\n",
    "    return col\n",
    "\n",
    "def get_trailing_number(s):\n",
    "    m = re.search(r'\\d+$', s)\n",
    "    return int(m.group()) if m else None\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "def union(lst1, lst2): \n",
    "    return list(set().union(lst1, lst2))\n",
    "\n",
    "def read(feather_dir, file, preds, cols, num_chunks):\n",
    "    indices = []\n",
    "    for filename in os.listdir(feather_dir):\n",
    "        for i,col in enumerate(cols):\n",
    "            if filename.startswith(col) :\n",
    "                pred = preds[i]\n",
    "                e = feather.read_dataframe(feather_dir + filename)\n",
    "                ind = e[pred].index\n",
    "                ind = np.add(ind, 1 + CHUNK_SIZE * get_trailing_number(filename.split('.')[0]))\n",
    "                if len(indices) > 0: # only for ands\n",
    "                    indices = union(indices, ind)\n",
    "                else:\n",
    "                    indices += list(ind)\n",
    "    df_labels = pd.read_csv(file, nrows=1)\n",
    "    df = pd.read_csv(file, skiprows=indices, header=0, names=df_labels.columns, index_col=False)\n",
    "    return df\n",
    "\n",
    "def read_cardinality_many(feather_dir, file, preds, cols, num_chunks):\n",
    "    indices = []\n",
    "    for i,col in enumerate(cols):\n",
    "        chunk_inds = []\n",
    "        for filename in os.listdir(feather_dir):\n",
    "            if filename.startswith(col) :\n",
    "                pred = preds[i]\n",
    "                e = feather.read_dataframe(feather_dir + filename)\n",
    "\n",
    "                ind = e[pred].index\n",
    "                ind = np.add(ind, CHUNK_SIZE * get_trailing_number(filename.split('.')[0]))\n",
    "                chunk_inds.extend(ind)\n",
    "        if len(indices) > 0: # only for ands\n",
    "            indices = intersection(indices, chunk_inds)\n",
    "        else:\n",
    "            indices = chunk_inds\n",
    "    return len(indices)\n",
    "\n",
    "def read_cardinality_one(feather_dir, file, preds, cols, num_chunks):\n",
    "    indices = []\n",
    "    total = 0\n",
    "    for filename in os.listdir(feather_dir):\n",
    "        for i,col in enumerate(cols):\n",
    "            if filename.startswith(col) :\n",
    "                pred = preds[i]\n",
    "                e = feather.read_dataframe(feather_dir + filename)\n",
    "                total += len(e[pred])\n",
    "    return total\n",
    "\n",
    "def read_fast(feather_dir, file, preds, cols, num_chunks):\n",
    "    \n",
    "    df_all = []\n",
    "    for i in range(0,num_chunks+1):\n",
    "        df_part = (feather.read_dataframe(f'{feather_dir}full{i}.f'))\n",
    "        for pred in preds:\n",
    "            df_part = df_part[pred]\n",
    "        df_all.append(df_part)\n",
    "    df = pd.concat(df_all, ignore_index=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chunk(df, chunk_id):\n",
    "    for c in df:\n",
    "#         df[c] = shrink_floats(df[c])\n",
    "        df[c] = shrink_integers(df[c])\n",
    "        df[c] = convert_to_categorical(df[c])\n",
    "        feather.write_dataframe(df[c].to_frame(),  f'{FEATHER_DIR}{c}{chunk_id}.f')\n",
    "    feather.write_dataframe(df, f'{FEATHER_DIR}full{chunk_id}.f')\n",
    "def write_chunks(file, chunk_size):\n",
    "    num_chunks = 0\n",
    "    for i, df in enumerate(pd.read_csv(file, iterator=True, chunksize=chunk_size)):\n",
    "        write_chunk(df, i)\n",
    "        num_chunks = i\n",
    "    return num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['raw_row_number'\n",
    "        ,'date','county_name','district','subject_race',\n",
    "        'subject_sex','department_name','type','violation','arrest_made',\n",
    "        'citation_issued','warning_issued','outcome','contraband_found',\n",
    "        'frisk_performed','search_conducted','search_person','search_basis',\n",
    "        'reason_for_stop','raw_race','raw_search_basis'\n",
    "       ]\n",
    "VALS = [32,'2009-07-01','San Diego County','San Onofre Inspection Facility',\n",
    "        'hispanic','male','California Highway Patrol','vehicular','Inspection / Scale Facility',\n",
    "        'NA','NA','NA','NA', 'NA','NA',False,False,'NA','Inspection / Scale Facility',\n",
    "        'Hispanic','Parole / Probation / Warrant'\n",
    "]\n",
    "\n",
    "PREDS = [lambda x: (x[COLS[0]] != VALS[0]),\n",
    "         lambda x: (x[COLS[1]] != VALS[1]),\n",
    "         lambda x: (x[COLS[2]] != VALS[2]),\n",
    "         lambda x: (x[COLS[3]] != VALS[3]),\n",
    "         lambda x: (x[COLS[4]] != VALS[4]),\n",
    "         lambda x: (x[COLS[5]] != VALS[5]),\n",
    "         lambda x: (x[COLS[6]] != VALS[6]),\n",
    "         lambda x: (x[COLS[7]] != VALS[7]),\n",
    "         lambda x: (x[COLS[8]] != VALS[8]),\n",
    "         lambda x: (~x[COLS[9]].isnull()),\n",
    "         lambda x: (~x[COLS[10]].isnull()),\n",
    "         lambda x: (~x[COLS[11]].isnull()),\n",
    "         lambda x: (~x[COLS[12]].isnull()),\n",
    "         lambda x: (~x[COLS[13]].isnull()),\n",
    "         lambda x: (~x[COLS[14]].isnull()),\n",
    "         lambda x: (x[COLS[15]] != VALS[15]),\n",
    "         lambda x: (x[COLS[16]] != VALS[16]),\n",
    "         lambda x: (~x[COLS[17]].isnull()),\n",
    "         lambda x: (x[COLS[18]] != VALS[18]),\n",
    "         lambda x: (x[COLS[19]] != VALS[19]),\n",
    "         lambda x: (x[COLS[20]] != VALS[20]),\n",
    "        ]\n",
    "\n",
    "POS_PREDS = [\n",
    "         lambda x: (x[COLS[0]] == VALS[0]),\n",
    "         lambda x: (x[COLS[1]] == VALS[1]),\n",
    "         lambda x: (x[COLS[2]] == VALS[2]),\n",
    "         lambda x: (x[COLS[3]] == VALS[3]),\n",
    "         lambda x: (x[COLS[4]] == VALS[4]),\n",
    "         lambda x: (x[COLS[5]] == VALS[5]),\n",
    "         lambda x: (x[COLS[6]] == VALS[6]),\n",
    "         lambda x: (x[COLS[7]] == VALS[7]),\n",
    "         lambda x: (x[COLS[8]] == VALS[8]),\n",
    "         lambda x: (x[COLS[9]].isnull()),\n",
    "         lambda x: (x[COLS[10]].isnull()),\n",
    "         lambda x: (x[COLS[11]].isnull()),\n",
    "         lambda x: (x[COLS[12]].isnull()),\n",
    "         lambda x: (x[COLS[13]].isnull()),\n",
    "         lambda x: (x[COLS[14]].isnull()),\n",
    "         lambda x: (x[COLS[15]] == VALS[15]),\n",
    "         lambda x: (x[COLS[16]] == VALS[16]),\n",
    "         lambda x: (x[COLS[17]].isnull()),\n",
    "         lambda x: (x[COLS[18]] == VALS[18]),\n",
    "         lambda x: (x[COLS[19]] == VALS[19]),\n",
    "         lambda x: (x[COLS[20]] == VALS[20]),\n",
    "        ]\n",
    "\n",
    "# COLS = ['issue_url','issue_title','body']\n",
    "# VALS = ['https://github.com/DungeonKeepers/DnDInventoryManager/issues/2', 'api', 'documentation']\n",
    "\n",
    "\n",
    "# POS_PREDS = [\n",
    "#              lambda x: (x[COLS[0]].str.contains(VALS[0])),\n",
    "#              lambda x: (x[COLS[1]].str.contains(VALS[1])),\n",
    "#              lambda x: (x[COLS[2]].str.contains(VALS[2])),\n",
    "#             ]\n",
    "                    \n",
    "# PREDS = [\n",
    "#          lambda x: (~x[COLS[0]].str.contains(VALS[0])),\n",
    "#          lambda x: (~x[COLS[1]].str.contains(VALS[1])),\n",
    "#          lambda x: (~x[COLS[2]].str.contains(VALS[2])),\n",
    "#         ]\n",
    "\n",
    "# COLS = ['LCLid','tstp','energy']\n",
    "# VALS = ['MAC000002','2012-10-12 00:30:00.0000000', 0]\n",
    "\n",
    "\n",
    "# POS_PREDS = [\n",
    "#              lambda x: (x[COLS[0]].str.contains(VALS[0])),\n",
    "#              lambda x: (x[COLS[1]].str.contains(VALS[1])),\n",
    "#              lambda x: (x[COLS[2]] == VALS[2]),\n",
    "#             ]\n",
    "                    \n",
    "# PREDS = [\n",
    "#          lambda x: (~x[COLS[0]].str.contains(VALS[0])),\n",
    "#          lambda x: (~x[COLS[1]].str.contains(VALS[1])),\n",
    "#          lambda x: (x[COLS[2]] != VALS[2]),\n",
    "#         ]\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 5_000_000\n",
    "FILE = 'datasets/github_issues.csv'\n",
    "FEATHER_DIR = 'datasets/github_issues/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 55.7 s, total: 1min 58s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_chunks = write_chunks(FILE, CHUNK_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043872\n",
      "CPU times: user 10 s, sys: 1.5 s, total: 11.5 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(read_cardinality_many(FEATHER_DIR, FILE, POS_PREDS[8:13], COLS[8:13], num_chunks))\n",
    "# print(read_cardinality_one(FEATHER_DIR, FILE, POS_PREDS[8:9], COLS[8:9], num_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(528503, 21)\n",
      "CPU times: user 6.14 s, sys: 1.05 s, total: 7.19 s\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_chunks = write_chunks(FILE, CHUNK_SIZE)\n",
    "df_read = read_fast(FEATHER_DIR, FILE, PREDS, COLS, num_chunks)\n",
    "\n",
    "print(df_read.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "CPU times: user 52.5 s, sys: 15.2 s, total: 1min 7s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_read = read(FEATHER_DIR, FILE, PREDS[0:3], COLS[0:3], num_chunks)\n",
    "print(df_read.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "CPU times: user 23.9 s, sys: 1.71 s, total: 25.6 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "indices = []\n",
    "df = pd.read_csv(FILE)\n",
    "\n",
    "for pred in PREDS[0:3]:\n",
    "    indices = df[pred].index\n",
    "    df.drop(indices, inplace=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "CPU times: user 23.6 s, sys: 1.4 s, total: 25 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_all = []\n",
    "\n",
    "for i, df_p in enumerate(pd.read_csv(FILE, iterator=True, chunksize=CHUNK_SIZE)):\n",
    "    for pred in PREDS[8:9]:\n",
    "        indices = df_p[pred].index\n",
    "        df_p.drop(indices, inplace=True)\n",
    "    df_all.append(df_p)\n",
    "\n",
    "df = pd.concat(df_all, ignore_index=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test across all columns (when is it worth running in this system)\n",
    "\n",
    "shrinking encodings of different type (approximate queries)\n",
    "\n",
    "ca_police 6.7GB\n",
    "full_files 802MB\n",
    "col_files 802 MB\n",
    "parse time 3min 36s\n",
    "\n",
    "\n",
    "\n",
    "PREDS[5:6] 22161713\n",
    "\n",
    "read_fast 8.66 s\n",
    "read 1min 24s\n",
    "read_chunks 2min 20s\n",
    "read_csv 2min 55s\n",
    "\n",
    "PREDS[4:10] 528503\n",
    "\n",
    "read_fast  6.58s\n",
    "read 2 min 7s\n",
    "read_chunks 2min 12s\n",
    "read_csv 3min 21s\n",
    "\n",
    "PREDS[0:20] 1\n",
    "\n",
    "read_fast 2.66 s\n",
    "read 6min 38s\n",
    "read_chunks 1min 59s\n",
    "read_csv 2min 58s\n",
    "\n",
    "\n",
    "github_issues 1min 27s\n",
    "\n",
    "PREDS[0:3] 1\n",
    "\n",
    "read_fast 51.9 s\n",
    "read 2min 56s\n",
    "read_chunks 49.9 s\n",
    "read_csv 1min 9s\n",
    "\n",
    "PREDS[0:1] 102077\n",
    "\n",
    "read_fast 2min 3s\n",
    "read 4min 43s\n",
    "read_chunks 1min 23s\n",
    "read_csv 2min 26s\n",
    "\n",
    "\n",
    "block_total 22.6 s\n",
    "\n",
    "PREDS [2:3] 183368\n",
    "\n",
    "read_fast 795ms\n",
    "read 29.5 s\n",
    "read_chunks 21.8 s\n",
    "read_csv 17.5 s\n",
    "\n",
    "PREDS [0:3] 1\n",
    "\n",
    "read_fast 1.92 s\n",
    "read 1min 13s\n",
    "read_chunks 26.3 s\n",
    "read_csv 29.6 s\n",
    "\n",
    "\n",
    "\n",
    "Queries\n",
    "\n",
    "ca_police[8] 82260698\n",
    "cardinality_one 234 ms\n",
    "cardinality_many 447 ms\n",
    "normal 2-3 min\n",
    "\n",
    "ca_police[8:10]\n",
    "cardinality_many 4.1s\n",
    "read_fast ~10 s\n",
    "normal ~2 min\n",
    "\n",
    "ca_police[8:11]\n",
    "cardinality_many 6.57s\n",
    "read_fast ~10 s\n",
    "normal ~2 min\n",
    "\n",
    "ca_police[8:13]\n",
    "cardinality_many 12.1s\n",
    "read_fast ~10 s\n",
    "normal ~2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatted\n",
      "RangeIndex(start=0, stop=4999999, step=1)\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "    \n",
    "df_all = []\n",
    "for i in range(0,num_chunks+1):\n",
    "    df_all.append(feather.read_dataframe(f'{FEATHER_DIR}full{i}.f'))\n",
    "df = pd.concat(df_all, ignore_index=True)\n",
    "print('concatted')\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
