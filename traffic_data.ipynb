{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import resource \n",
    "%load_ext memory_profiler\n",
    "\n",
    "file = 'block_total.csv'\n",
    "total_lines = 17_703_908\n",
    "#LCLid,tstp,energy(kWh/hh)\n",
    "pred = (lambda x: (x['energy(kWh/hh)'].astype('float')) < 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ideas**\n",
    "\n",
    "Set max memory usage and have chunksize dynamically adjust\n",
    "\n",
    "parse out columns and pickle so can do fast query on column to get list of rows for selection\n",
    "\n",
    "however, then requires two file reads and still have to seek through all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg line bytes 175.20800000000003\n",
      "MEM estimate 3101866312.8640003\n",
      "ltr 1496187\n",
      "ltr 1127782\n",
      "ltr 932692\n",
      "ltr 826136\n",
      "ltr 731836\n",
      "ltr 692819\n",
      "ltr 670989\n",
      "ltr 644524\n",
      "ltr 610125\n",
      "ltr 594049\n",
      "ltr 575115\n",
      "ltr 545083\n",
      "ltr 528607\n",
      "ltr 500047\n",
      "ltr 480376\n",
      "ltr 453198\n",
      "ltr 411902\n",
      "ltr 374286\n",
      "ltr 368200\n",
      "ltr 352163\n",
      "ltr 336456\n",
      "ltr 328702\n",
      "ltr 324215\n",
      "ltr 320054\n",
      "ltr 295931\n",
      "ltr 288141\n",
      "ltr 278728\n",
      "ltr 264997\n",
      "ltr 251829\n",
      "ltr 242174\n",
      "ltr 229764\n",
      "ltr 218553\n",
      "ltr 211013\n",
      "ltr 206386\n",
      "ltr 201413\n",
      "ltr 191528\n",
      "ltr 189142\n",
      "ltr 181505\n",
      "ltr 175871\n",
      "ltr 51291\n",
      "Total Chunks: 40\n",
      "(1324834, 3)\n",
      "peak memory: 249.11 MiB, increment: 175.82 MiB\n",
      "CPU times: user 13.1 s, sys: 1.72 s, total: 14.9 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "SAMPLE_SIZE = 100\n",
    "MAX_MEM_MB = 250\n",
    "MAX_MEM_BYTES = 1024*1024*MAX_MEM_MB\n",
    "\n",
    "iterator = pd.read_csv(file, iterator=True)\n",
    "\n",
    "df_head = iterator.get_chunk(size=SAMPLE_SIZE)\n",
    "\n",
    "avg_line_bytes = 1.1 * df_head.memory_usage(deep=True).sum()/SAMPLE_SIZE\n",
    "print(f'avg line bytes {avg_line_bytes}')\n",
    "\n",
    "total_mem_estimate = total_lines * avg_line_bytes\n",
    "print(f'MEM estimate {total_mem_estimate}')\n",
    "\n",
    "df_read_size = 0\n",
    "lines_read = SAMPLE_SIZE - 1\n",
    "\n",
    "dfs = [df_head]\n",
    "chunk_count = 0\n",
    "\n",
    "iterator = pd.read_csv(file, iterator=True)\n",
    "\n",
    "\n",
    "while lines_read < total_lines:\n",
    "    chunk_count += 1\n",
    "    lines_to_read = min(int((MAX_MEM_BYTES - df_read_size) / avg_line_bytes), total_lines-lines_read)\n",
    "    print(f\"ltr {lines_to_read}\")\n",
    "    df_chunk = iterator.get_chunk(size=lines_to_read)\n",
    "    \n",
    "    indices = df_chunk[pred].index\n",
    "    df_chunk.drop(indices, inplace=True)\n",
    "    \n",
    "#     chunk_mem = df_chunk.memory_usage(deep=True).sum() #actual\n",
    "    chunk_mem = (lines_to_read - len(indices))  * avg_line_bytes #fast estimate\n",
    "    \n",
    "    dfs.append(df_chunk)\n",
    "    \n",
    "    lines_read += lines_to_read\n",
    "    df_read_size += chunk_mem\n",
    "\n",
    "\n",
    "df_total = pd.concat(dfs)\n",
    "\n",
    "print(f'Total Chunks: {chunk_count}')\n",
    "# chunk_mem = df_total.memory_usage(deep=True).sum()\n",
    "# print(f'Total Mem Usage  {chunk_mem/(1024*1024)} MB')\n",
    "print(df_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354 chunks of size 50000 read\n",
      "(1324734, 3)\n",
      "peak memory: 377.87 MiB, increment: 44.68 MiB\n",
      "CPU times: user 17.4 s, sys: 2.93 s, total: 20.3 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "CHUNKSIZE = 50_000\n",
    "dfs = []\n",
    "\n",
    "for i, df in enumerate(pd.read_csv(file, iterator=True, chunksize=CHUNKSIZE)):\n",
    "    indices = df[pred].index\n",
    "#     chunk_mem = df.memory_usage(deep=True).sum()\n",
    "    df.drop(indices, inplace=True)\n",
    "#     print(f'chunkmem {chunk_mem/(1024*1024)} MB')\n",
    "    dfs.append(df)\n",
    "\n",
    "df_total = pd.concat(dfs)\n",
    "\n",
    "# chunk_mem = df_total.memory_usage(deep=True).sum()\n",
    "print(f'{i} chunks of size {CHUNKSIZE} read')\n",
    "# print(f'Total Current Mem Usage  {chunk_mem/(1024*1024)} MB')\n",
    "\n",
    "print(df_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1324734, 3)\n",
      "peak memory: 1950.93 MiB, increment: 337.46 MiB\n",
      "CPU times: user 6.94 s, sys: 1.5 s, total: 8.44 s\n",
      "Wall time: 8.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# print(list(indices))\n",
    "df = pd.read_csv(file, skiprows=ls)\n",
    "\n",
    "# indices = df[pred].index\n",
    "\n",
    "# chunk_mem = df.memory_usage(deep=True).sum()\n",
    "# print(chunk_mem)\n",
    "\n",
    "# df.drop(indices, inplace=True)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 78.72 MiB, increment: 6.03 MiB\n",
      "CPU times: user 53 ms, sys: 34.9 ms, total: 87.8 ms\n",
      "Wall time: 223 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "CHUNKSIZE = 600_000\n",
    "dfs = []\n",
    "\n",
    "iterator = pd.read_csv(file, iterator=True, chunksize=CHUNKSIZE)\n",
    "\n",
    "\n",
    "#     indices = df[pred].index\n",
    "# #     chunk_mem = df.memory_usage(deep=True).sum()\n",
    "#     df.drop(indices, inplace=True)\n",
    "# #     print(f'chunkmem {chunk_mem/(1024*1024)} MB')\n",
    "#     dfs.append(df)\n",
    "\n",
    "# df_total = pd.concat(dfs)\n",
    "\n",
    "# chunk_mem = df_total.memory_usage(deep=True).sum()\n",
    "# print(f'{i} chunks of size {CHUNKSIZE} read')\n",
    "# print(f'Total Current Mem Usage  {chunk_mem/(1024*1024)} MB')\n",
    "\n",
    "# print(df_total.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
